{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:89: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:31: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nWARNING:tensorflow:From /home/vuthede/anaconda3/envs/rnn/lib/python3.7/site-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From ../../PRNet/predictor.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:91: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:91: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:94: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n\nWARNING:tensorflow:From ../../PRNet/predictor.py:75: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nINFO:tensorflow:Restoring parameters from ../../PRNet/Data/net-data/256_256_resfcn256_weight\n"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(0, \"../../PRNet\")\n",
    "from api import PRN\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # cpu\n",
    "prn = PRN(is_dlib = True, prefix=\"../../PRNet\")\n",
    "from utils.render import render_texture\n",
    "from utils.rotate_vertices import frontalize\n",
    "from utils.estimate_pose import estimate_pose\n",
    "import numpy as np\n",
    "import dlib\n",
    "sys.path.insert(0, \"../../deepfeatinterp\")\n",
    "import alignface\n",
    "\n",
    "import alignface\n",
    "\n",
    "import time\n",
    "def warp_face(img_in, img_ref):\n",
    "    #img_in is HD image\n",
    "    # img_ref is synthesis image\n",
    "    # Calculate position map and get pose and rotation matrix\n",
    "    pos1 = prn.process(img_in) \n",
    "    vertices1 = prn.get_vertices(pos1)\n",
    "    cam_mat1, pose1, R1 = estimate_pose(vertices1)\n",
    "    pos2 = prn.process(img_ref) \n",
    "    vertices2 = prn.get_vertices(pos2)\n",
    "    cam_mat2, pose2, R2 = estimate_pose(vertices2)\n",
    "\n",
    "    # Rotation 3D vertices\n",
    "    warp_vertices = np.matmul(np.matmul(vertices2,R2), np.linalg.inv(R1)) \n",
    "\n",
    "    # Do translation\n",
    "    center2_warp_pt = np.mean(warp_vertices, axis=0)\n",
    "    center1_pt = np.mean(vertices1, axis=0)\n",
    "    warp_vertices = warp_vertices - (center2_warp_pt - center1_pt)\n",
    "\n",
    "    # Render new synthesis image after doing transformation\n",
    "    # t1 = time.time()\n",
    "    texture_ref = cv2.remap(img_ref/255.0, pos2[:,:,:2].astype(np.float32), None, interpolation=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT,borderValue=(0))\n",
    "    # print(\"Texture from pos map: \", time.time()-t1)\n",
    "\n",
    "    [h, w, c] = img_ref.shape\n",
    "    # t1 = time.time()\n",
    "    \n",
    "    color = prn.get_colors_from_texture(texture_ref)\n",
    "    # print(\"Get color of texture: \", time.time()-t1)\n",
    "    \n",
    "    color_mask = np.ones((warp_vertices.shape[0], 1))\n",
    "    t1 = time.time()\n",
    "    new_image = render_texture(warp_vertices.T, color.T, prn.triangles.T, h, w, c = 3)\n",
    "    print(\"Time render: \", time.time()-t1)\n",
    "    facemask = render_texture(warp_vertices.T, color_mask.T, prn.triangles.T, h, w, c = 3)\n",
    "\n",
    "    # Using seamlessCloning to blending images\n",
    "    vis_ind = np.argwhere(facemask>0)\n",
    "    vis_min = np.min(vis_ind, 0)\n",
    "    vis_max = np.max(vis_ind, 0)\n",
    "    center = (int((vis_min[1] + vis_max[1])/2+0.5), int((vis_min[0] + vis_max[0])/2+0.5))\n",
    "    output = cv2.seamlessClone((new_image*255).astype(np.uint8),(img_in).astype(np.uint8), (facemask*255).astype(np.uint8), center, cv2.NORMAL_CLONE)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_hd = cv2.VideoCapture(\"../cropobamafullhd10s.mp4\")\n",
    "cap_syn = cv2.VideoCapture(\"../resultobamafullhd10s.mp4\")\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    i +=1\n",
    "\n",
    "    if i%15:\n",
    "        ret1, hd = cap_hd.read()\n",
    "        ret2, syn = cap_syn.read()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            print(\"Quit\")\n",
    "            out.release()\n",
    "            break\n",
    "\n",
    "        warp, transform = warp_face_2D(hd, syn)\n",
    "\n",
    "        concat = np.hstack([hd, syn,transform, warp])\n",
    "\n",
    "        cv2.imshow(\"hd\", hd)\n",
    "        cv2.imshow(\"syn\", syn)\n",
    "        cv2.imshow(\"warp\", warp)\n",
    "        cv2.imshow(\"transform\", transform)\n",
    "        cv2.imshow(\"concat\", concat)\n",
    "        print(concat.shape)\n",
    "        out.write(concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        k = cv2.waitKey(0)\n",
    "        if k==27:\n",
    "            out.release()\n",
    "            break\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "57\n"
    }
   ],
   "source": [
    "import random\n",
    "quality_ranges = [(15, 75)]\n",
    "for qr in quality_ranges:\n",
    "    quality = int(random.random() * (qr[1] - qr[0]) + qr[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/vuthede/Downloads/shape_predictor_81_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(model_path)\n",
    "face_d,face_p=alignface.load_face_detector(predictor_path=\"/home/vuthede/AI/deepfeatinterp/models/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "\n",
    "def get_landmark(im1):\n",
    "    det0 = detector(im1, 0)[0]\n",
    "    shape = predictor(im1, det0)\n",
    "    landmarks = np.matrix([[p.x, p.y] for p in shape.parts()])\n",
    "    landmarks = np.array(landmarks)\n",
    "    return landmarks\n",
    "\n",
    "def draw_landmark(im, l, color =(0,255,0)):\n",
    "    im_ = im.copy()\n",
    "    for i in l:\n",
    "        cv2.circle(im_, (i[0],i[1]), 3,color, -1)\n",
    "\n",
    "    return im_\n",
    "\n",
    "def warp_face_2D(img_in, img_ref):\n",
    "    cv2.imwrite(\"tmp.png\", img_ref)\n",
    "    cv2.imwrite(\"tmp2.png\", img_in)\n",
    "\n",
    "    lm1 = get_landmark(img_ref)\n",
    "    lm2 = get_landmark(img_in)\n",
    "    # template,original=alignface.detect_landmarks(\"tmp.png\",face_d,face_p)\n",
    "    # template2,original2=alignface.detect_landmarks(\"tmp2.png\",face_d,face_p)\n",
    "\n",
    "    M,loss=alignface.fit_face_landmarks(lm1[:,::-1],lm2[:,::-1], landmarks=list(range(81)), scale_landmarks=[0,16],location_landmark=30,image_dims=img_in.shape[:2])\n",
    "\n",
    "    warp_image = alignface.warp_to_template(img_in,M,border_value=(0.5,0.5,0.5),image_dims=img_in.shape)\n",
    "\n",
    "    cv2.imshow(\"warp image raw:\", warp_image)\n",
    "\n",
    "     # Find convex hull\n",
    "    hull1 = []\n",
    "    hull2 = []\n",
    "\n",
    "   \n",
    "\n",
    "    # template = np.array(template[:,::-1]).astype(int)\n",
    "    # template2 = np.array(template2[:,::-1]).astype(int)\n",
    "\n",
    "    show = draw_landmark(img_ref, lm1)\n",
    "    cv2.imshow(\"Lmk:\", show)\n",
    "\n",
    "    # print(template)\n",
    "    # print(template.shape)\n",
    "\n",
    "    hullIndex = cv2.convexHull(lm1, returnPoints = False)          \n",
    "    for i in range(0, len(hullIndex)):\n",
    "        hull1.append(lm1[int(hullIndex[i])])\n",
    "        hull2.append(lm2[int(hullIndex[i])])\n",
    "    \n",
    "    hull1 = np.array(hull1)\n",
    "    hull2 = np.array(hull2)\n",
    "\n",
    "    mouth_mask = np.zeros(img_ref.shape, img_ref.dtype)\n",
    "    cv2.fillPoly(mouth_mask, [hull2], (255, 255, 255))\n",
    "    cv2.imshow(\"mask:\", mouth_mask)\n",
    "\n",
    "    r = cv2.boundingRect(np.float32(hull2))    \n",
    "    center = ((r[0]+int(r[2]/2), r[1]+int(r[3]/2)))\n",
    "    clone = cv2.seamlessClone(warp_image, img_in, mouth_mask, center, cv2.NORMAL_CLONE)\n",
    "\n",
    "    return clone, warp_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\n(256, 1024, 3)\nQuit\n"
    }
   ],
   "source": [
    "cap_hd = cv2.VideoCapture(\"../cropobamafullhd10s.mp4\")\n",
    "cap_syn = cv2.VideoCapture(\"../resultobamafullhd10s.mp4\")\n",
    "out = cv2.VideoWriter('./data_playing_2d.mp4',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (256*4, 256))\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    i +=1\n",
    "\n",
    "    if i%15:\n",
    "        ret1, hd = cap_hd.read()\n",
    "        ret2, syn = cap_syn.read()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            print(\"Quit\")\n",
    "            out.release()\n",
    "            break\n",
    "\n",
    "        warp, transform = warp_face_2D(hd, syn)\n",
    "\n",
    "        concat = np.hstack([hd, syn,transform, warp])\n",
    "\n",
    "        cv2.imshow(\"hd\", hd)\n",
    "        cv2.imshow(\"syn\", syn)\n",
    "        cv2.imshow(\"warp\", warp)\n",
    "        cv2.imshow(\"transform\", transform)\n",
    "        cv2.imshow(\"concat\", concat)\n",
    "        print(concat.shape)\n",
    "        out.write(concat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        k = cv2.waitKey(0)\n",
    "        if k==27:\n",
    "            out.release()\n",
    "            break\n",
    "\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1024"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "256*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "New batch\nNew batch\nNew batch\nNew batch\nNew batch\n"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def cut_batch_data(video, output_dir):\n",
    "    if not os.path.isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    cap = cv2.VideoCapture(video)\n",
    "    batch = 0\n",
    "    i = 0\n",
    "    while True:\n",
    "        ret, img = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if i%(60*30*2)==0:\n",
    "            batch +=1\n",
    "            out = cv2.VideoWriter(f'./{output_dir}/{batch}.mp4',cv2.VideoWriter_fourcc('M','J','P','G'), 30, (256, 256))\n",
    "            print(\"New batch\")\n",
    "\n",
    "        out.write(img)\n",
    "        i+=1\n",
    "\n",
    "    out.release()\n",
    "\n",
    "\n",
    "cut_batch_data(\"./obama_concat_data.mp4\",output_dir=\"batches\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595299454302",
   "display_name": "Python 3.7.0 64-bit ('rnn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}